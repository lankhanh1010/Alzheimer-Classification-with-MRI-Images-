| Model                         | Base Model Frozen Layers        | Custom Layers Added                                                                                                                                                               | Dropout              | Batch Normalization         | Optimizer                      | Final Activation | Epoch | Batch Size | Data Handling Strategy             |
|--------------------------------|--------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|----------------------|-----------------------------|--------------------------------|------------------|-------|------------|----------------------------------|
| InceptionV3                   | All layers frozen              | Flatten → Dense (1024, ReLU) → Dropout (0.3) → Dense (512, ReLU) → Dropout (0.3) → Dense (256, ReLU) → Dropout (0.3) → Dense (softmax, numClasses) | 0.3 for each Dense layer | No                          | SGD (lr=0.0001, momentum=0.9)  | Softmax          | 200   | 50         | Train on unbalanced dataset     |
| Classifier for InceptionV3     | N/A                            | Dense (256, ReLU) → BatchNorm → Dropout (0.3) → Dense (128, ReLU) → BatchNorm → Dropout (0.3) → Dense (softmax, numClasses)                | 0.3 for each Dense layer | Yes (After each Dense layer) | Adam (lr=0.0001)               | Softmax          | 100   | 50         | Train on SMOTE-balanced dataset |
| DenseNet201                    | All layers frozen              | GlobalAveragePooling → Dense (1920, ReLU, L2=0.01) → Dropout (0.3) → Dense (1024, ReLU, L2=0.01) → Dropout (0.4) → Dense (512, ReLU, L2=0.01) → Dropout (0.5) → Dense (softmax, numClasses) | 0.3, 0.4, 0.5         | Yes (After each Dense layer) | SGD (lr=0.0001, momentum=0.9)  | Softmax          | 200   | 50         | Train on unbalanced dataset     |
| Classifier for DenseNet201     | N/A                            | Dense (256, ReLU, L2=0.001) → Dropout (0.3) → Dense (128, ReLU) → Dropout (0.3) → Dense (softmax, numClasses)                              | 0.3 for each Dense layer | Yes (After each Dense layer) | Adam (lr=0.0001)               | Softmax          | 100   | 64         | Train on SMOTE-balanced dataset |
| ResNet152V2                    | Last 50 layers fine-tuned      | Flatten → Dense (1024, ReLU) → Dropout (0.3) → Dense (512, ReLU) → Dropout (0.3) → Dense (256, ReLU) → Dropout (0.3) → Dense (softmax, numClasses) | 0.3 for each Dense layer | No                          | SGD (lr=1e-5, momentum=0.9)    | Softmax          | 200   | 50         | Train on unbalanced dataset     |
| Classifier for ResNet152V2     | N/A                            | Dense (8192, ReLU) → BatchNorm → Dropout (0.3) → Dense (4096, ReLU) → BatchNorm → Dropout (0.3) → Dense (2048, ReLU) → BatchNorm → Dropout (0.3) → Dense (1024, ReLU) → BatchNorm → Dropout (0.3) → Dense (softmax, numClasses) | 0.3 for each Dense layer | Yes (After each Dense layer) | Adam (lr=0.0001)               | Softmax          | 100   | 50         | Train on SMOTE-balanced dataset |
